{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ed41934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5290e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clean:\n",
    "    \"\"\"\n",
    "    This class is to clean the data (which are cropped by filtered_data variable in step 1).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cropped_file, clean_file):\n",
    "        \"\"\"\n",
    "        :param cropped_file: the csv file which is cropped by step1_cropping_data_from_larger_dataset.py\n",
    "        :param clean_file: this is the new csv file(which will be created) after cleaning the data.\n",
    "        \"\"\"\n",
    "        self.Stations = {\n",
    "            '188': 'AURN Bristol Centre',\n",
    "            '203': 'Brislington Depot',\n",
    "            '206': 'Rupert Street',\n",
    "            '209': 'IKEA M32',\n",
    "            '213': 'Old Market',\n",
    "            '215': 'Parson Street School',\n",
    "            '228': 'Temple Meads Station',\n",
    "            '270': 'Wells Road',\n",
    "            '271': 'Trailer Portway P&R',\n",
    "            '375': 'Newfoundland Road Police Station',\n",
    "            '395': \"Shiner's Garage\",\n",
    "            '452': 'AURN St Pauls',\n",
    "            '447': 'Bath Road',\n",
    "            '459': 'Cheltenham Road \\ Station Road',\n",
    "            '463': 'Fishponds Road',\n",
    "            '481': 'CREATE Centre Roof',\n",
    "            '500': 'Temple Way',\n",
    "            '501': 'Colston Avenue',\n",
    "        }\n",
    "\n",
    "        self.clean_list = []\n",
    "        self.crop_csv = cropped_file\n",
    "        self.clean_csv_file = clean_file\n",
    "\n",
    "    def cleaning(self):\n",
    "        \"\"\"\n",
    "        Processing the data cleaning. Write data after checking mismatched lines(SiteID and location).\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # variable declaration and initialization\n",
    "        i = 1\n",
    "        remove_row_count = 0\n",
    "        write_row_count = 1\n",
    "\n",
    "        # read the data from step1_cropping_data_from_larger_dataset.csv and write it to cleaned_data.csv file\n",
    "        with open(self.crop_csv, 'r') as cropped_csv, open(self.clean_csv_file, 'w') as clean_csv:\n",
    "            for line in cropped_csv:\n",
    "                if i == 1:\n",
    "                    clean_csv.write(line)  # header line\n",
    "                else:\n",
    "                    split_row = line.split(\",\")\n",
    "\n",
    "                    # filtering the data(SiteID and Location) and only the matched data are written to clean.csv file\n",
    "                    if (split_row[4], split_row[17]) in self.Stations.items():\n",
    "                        clean_csv.write(line)\n",
    "                        write_row_count += 1\n",
    "                    else:\n",
    "                        # mismatched data are put to the removed count\n",
    "                        remove_row_count += 1\n",
    "                i += 1\n",
    "\n",
    "        print(\"The number of mismatches found and lines removed: \" + str(remove_row_count))\n",
    "        print(\"The number of lines written to cleaned_data.csv file: \" + str(write_row_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cf6ac8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of mismatches found and lines removed: 2029\n",
      "The number of lines written to cleaned_data.csv file: 14435\n",
      "Total execution time taken(seconds): 0:00:00.106982\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.datetime.now()\n",
    "try:\n",
    "    # cleaning the data (removed which are the duplicated, invalid SiteID and mismatching in SiteID and Location.).\n",
    "    obj_clean = Clean(\"cropped_records_with_filtered_date.csv\", \"cleaned_data.csv\")\n",
    "    obj_clean.cleaning()\n",
    "\n",
    "except BaseException as error:\n",
    "    print(f\"Error message: {error}\")\n",
    "\n",
    "print('Total execution time taken(seconds): ' + str(datetime.datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f953f1db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
